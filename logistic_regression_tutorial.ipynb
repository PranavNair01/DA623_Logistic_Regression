{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the coursework in DA623 (Computing with Signals) IIT Guwahati, this notebook provides a tutorial on the famous logistic regression algorithm used for classification tasks.\n",
    "\n",
    "This tutorial assumes very elementary background knowledge of the reader, and explains concepts from the basics as much as possible. However, it is highly recommended that the reader familiarizes themselves with linear regression before jumping into this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest one line definition of logistic regression would be:\n",
    "\n",
    "`\"Logistic Regression is a supervised learning algorithm for classification\"`\n",
    "\n",
    "But, let's break down every term for a better understanding:\n",
    "- `supervised learning`: In machine learning, typically 3 types of tasks are encountered:\n",
    "    1. Supervised Learning\n",
    "    2. Unsupervised Learning\n",
    "    3. Reinforcement Learning\n",
    "\n",
    "    The most commonly encountered being supervised learning. In supervised learning, we are provided data ($x$) along with labels ($y$). Based on these, we aim to learn the relationship between $x$ and $y$. More formally, we try to find the best hypothesis function $\\hat{f}$ that fits the data, i.e., $y = \\hat{f}(x)$. We call $\\hat{f}$ as hypothesis function, since we assume the type of relation between $x$ and $y$, say polynomial. Note: the actual underlying relationship between $x$ and $y$ is unknown to us, and may not be the same type as the hypothesis function. This function is also called target function $f$ such that $y = f(x)$. This might've been a hyperbola instead of polynomial. We just dont know.\n",
    "\n",
    "- `classification`: The labels provided in supervised learning may be a continuous variable (eg: temperature of a city) or a discrete variable (eg: blood group types). When the labels are continuous, we call it a regression algorithm. On the other hand, when the variables are discrete, we call it a classification algorithm. In our case, we are performing binary classification, i.e., our labels are discrete and further has only 2 values (eg: male or female, true or false).\n",
    "\n",
    "Note: Logistic Regression unlike the name suggests is always a classification algorithm, not a regression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, let us consider the case when we have 1D data ($x$). Being an instance of binary classification, these data points each would have one of the 2 labels, namely ($y = 0$ and $y = 1$).\n",
    "\n",
    "Let us try to fit a straight line, say $\\hat{y} = w_0 + w_1x$ to the given data, similar to linear regression. The situation would look something like this:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/linear_regression.png\">\n",
    "</div>\n",
    "\n",
    "However, in our case of binary classification, we would like $y$ to be between 0 and 1. Hence, we use a sigmoid function. A sigmoid function looks as follows:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/sigmoid_function.png\">\n",
    "</div>\n",
    "\n",
    "Hence, on setting $z = \\hat{y} = w_0 + w_1x$, we get $z \\to 1$ for large values of $\\hat{y}$, and $z \\to 0$ for small values of $\\hat{y}$. Accordingly, if $\\hat{y} = \\phi(z)$, where $z = w_0 + w_1x$ represents the hypothesis function for logistic regression on the given data $x$, we get the following:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/logistic_regression.png\">\n",
    "</div>\n",
    "\n",
    "But again one piece of the puzzle remains left out, the labels $\\hat{y}$ must be 0 or 1, not between 0 and 1. Here comes the concept of decision boundary. More formally, the hypothesis function $\\phi(z)$ is not actually the labels $\\hat{y}$, but is the probability of a given data point being 1, i.e., $\\phi(z) = \\phi(w_0 + w_1x) = P(\\hat{y} = 1 | x)$.\n",
    "\n",
    "Thereby, we set a threshold probability, say $p_0$ also known as the decision boundary. Whenever we get $\\phi(z) \\ge p_0$ for a point $x$, we classify it as 1, i.e., $\\hat{y} = \\hat{f}(x) = 1$, else if $\\phi(z) < p_0$, we set $\\hat{y} = \\hat{f}(x) = 0$.\n",
    "\n",
    "For example, with $p_0 = 0.5$, the decision boundary looks as follows (Note: Axes are visible only in VS code light mode):\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/decision_boundary.webp\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Interesting Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression can be used for binary classification of multi dimensional. For example, we get the following decision boundary for 2D data, where $x1$ and $x2$ are the two dimensions, and blue circle represents label $y = 0$ and red cross represents label $y = 1$:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/binary_2d.png\">\n",
    "</div>\n",
    "\n",
    "In fact, logistic regression can also be used to fit data points belonging to more than 3 classes. Consider a label $\\hat{y}$ can be $y_1, y_2,$ or $y_3$. Then for every data point $x$ we fit 3 decision boundaries using $p_0^1, p_0^2, p_0^3$ on hypothesis functions $\\phi_1(x), \\phi_2(x), \\phi_3(x)$, i.e., $P(y_1 = 1|x), P(y_2 = 1|x), P(y_3 = 1|x)$ respectively. Thereby, the label $\\hat{y}$ for $x$ is that with largest probability. We can visualize in 2D as follows:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/multiclass_2d.png\">\n",
    "</div>\n",
    "\n",
    "Last but not least, logistic regression can also learn non linear decision boundaries. Suppose $z = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^2$ for 2D data, we get the following decision boundary:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/non_linear_boundary.jpeg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backtracking to our original problem at hand, i.e., logistic regression for binary classification, we get the following equations for computing hypothesis function and labels for n dimensional data:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=0}^n w_ix_i = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = \\textbf{w}^T  \\textbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-\\textbf{w}^T  \\textbf{x}}}\n",
    "$$\n",
    "\n",
    "Therefore, for $i$ th data point, say $x_i$, we get label as:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\hat{f}(x_i) = \\begin{cases} 1, &\\quad\\text{if }\\phi(\\textbf{w}^T  \\textbf{x}_i) \\ge p_0\\\\ 0, &\\quad\\text{if }\\phi(\\textbf{w}^T  \\textbf{x}_i) < p_0\\\\ \\end{cases}\n",
    "$$\n",
    "\n",
    "where $p_0$ is a threshold probability used to define the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get different hypothesis function, and as a result different decision boundaries for different choices of weights $\\textbf{w}$. As a result, it becomes necessary to choose an appropriate $\\textbf{w}$ so that we get accurate prediction of labels by our logistic regression classifier.\n",
    "\n",
    "Consequently, we define a cost function $J(w)$ such that minimizing it leads to better accuracy.\n",
    "\n",
    "An obvious candidate for $J(w)$ is the famous root mean squared error loss function used in linear regression, i.e:\n",
    "\n",
    "$$\n",
    "Cost = J(\\textbf{w}) = \\frac{1}{2N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is the true label and $\\hat{y}_i$ in this discussion is the likelihood probability obtained using the sigmoid function by our logistic regression classifier.\n",
    "\n",
    "However, the above cost function may lead to suboptimal results during gradient descent (shortly explained ahead) due to non convexity.\n",
    "\n",
    "Consequently, we adopt the following cost function:\n",
    "\n",
    "$$\n",
    "Cost = J(\\textbf{w}) = -\\sum_{i=1}^N (y_i\\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i))\n",
    "$$\n",
    "\n",
    "It can be seen that the above indeed works. \n",
    "\n",
    "When $y_i = 1$, Cost = $-\\log(\\hat{y}_i)$. Cost tends to 0 as $\\hat{y}_i$ tends to 1. Similarly, cost tends to $\\infty$ as $\\hat{y}_i$ tends to 0.\n",
    "\n",
    "When $y_i = 0$, Cost = $-\\log(1 - \\hat{y}_i)$. Cost tends to 0 as $\\hat{y}_i$ tends to 0. Similarly, cost tends to $\\infty$ as $\\hat{y}_i$ tends to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMmp1dmZhcml1N2xscXZldHVnNW43d21lemNueXhzNG83MmR5OTdnNCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/lPF1CyJXXcTZmUrP2J/giphy.gif\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost there! Now that we have our cost function, we would like to find weights $\\textbf{w}$ such that our cost function is minimized.\n",
    "\n",
    "The gradient descent algorithm does exactly that. A convex looks for 2D data looks as follows (ignore the axes for now):\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/convex_function.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a function has only 1 local minima which is also the global minima. Now, if we take a particular point, the gradient $\\nabla\\textbf{J} = \\left[\\frac{\\partial\\textbf{J}}{\\partial w_1}, \\frac{\\partial\\textbf{J}}{\\partial w_2}\\right]$ gives the direction of steepest accent. Consequently, updating $\\textbf{w}$ in the direction of $-\\nabla\\textbf{J}$ results in the highest decrease of the cost function. So, we perform the following until we reach the global maxima:\n",
    "\n",
    "$$\n",
    "\\forall \\,i\\,\\  w_i = w_i - \\alpha\\frac{\\partial\\textbf{J}}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "where $i$ is the $i$ th dimension, and we perform all updations simultaneously.\n",
    "\n",
    "In the above equation $\\alpha$ represents the learning rate parameter. For small values of $\\alpha$, gradient descent requires too much time (iterations) until it converges to the global minima. On the other hand, for large values of $\\alpha$, gradient descent may result in a very big updation of $w_i$ in the direction of the global minima, that it may overshoot the minima, and result in divergence to a high value of cost.\n",
    "\n",
    "Suppose, a function is non convex, it looks something like follows:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/non_convex_function.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result different starting points may lead to different local minimas (possibly not the global minima).\n",
    "\n",
    "Hence, it was necessary to choose a different cost function for logistic regression, since root mean square error is not a convex function for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExZmFwODNhNHJvdmw3OTl0a3Y3N2cyeXl0c2g4Z2tidm81dW93YWZxZiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Fgl9vJp70G6YJlVOHH/giphy.gif\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
